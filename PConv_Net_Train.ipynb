{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import v2\n",
    "import os\n",
    "import time\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32\n",
    "print_every = 1000\n",
    "image_dim = 512\n",
    "\n",
    "device = 'cuda' if (USE_GPU and torch.cuda.is_available()) else 'cpu'\n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't need to run this unless you are computing for a new dataset\n",
    "\n",
    "def calc_mean_std(loader):\n",
    "    '''\n",
    "    Calculates the Mean and Std of image dataset for each input channel.\n",
    "\n",
    "    Assumptions: The dataset has not been Transformed in any way\n",
    "    other than torchvision.transforms.ToTensor()\n",
    "\n",
    "    Input: Dataloader using minibatches of shape N x C x H x W\n",
    "\n",
    "    Output: Two C-vectors indicating mean and std\n",
    "    '''\n",
    "    avg = 0\n",
    "    var = 0\n",
    "    for e in range(1): #only 1 epoch\n",
    "        #for t, (x, y) in enumerate(loader):\n",
    "        for t, x in enumerate(loader):\n",
    "            # t = batch number (ex. 0-999 for N = 50,000 and loader batch_size = 50)\n",
    "            # x = 4D tensor batch data (N x C x H x W)\n",
    "            # y = labels/target for batch\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            #y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            this_avg = torch.mean(x, dim=(0,2,3))\n",
    "            if t == 0:\n",
    "                avg = this_avg\n",
    "            else:\n",
    "                avg = (t/(t+1))*avg + (1/(t+1))*this_avg\n",
    "\n",
    "    avg_broad = avg.reshape((1,3,1,1))\n",
    "    for e in range(1):\n",
    "        #for t, (x, y) in enumerate(loader):\n",
    "        for t, x in enumerate(loader):\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            zero_mean = x - avg_broad\n",
    "            this_var = torch.mean(torch.mul(zero_mean, zero_mean), dim=(0,2,3))\n",
    "            if t == 0:\n",
    "                var = this_var\n",
    "            else:\n",
    "                var = (t/(t+1))*var + (1/(t+1))*this_var\n",
    "\n",
    "\n",
    "    return avg, torch.sqrt(var)\n",
    "\n",
    "#cifar100_mean, cifar100_std = calc_mean_std(loader_sandbox)\n",
    "#print(\"Mean: \" + str(cifar100_mean))\n",
    "#print(\"Std: \" + str(cifar100_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilationTransform(nn.Module):\n",
    "    def __init__(self, min_kernel, max_kernel) -> None:\n",
    "        super().__init__()\n",
    "        self.min_kernel = min_kernel\n",
    "        self.max_kernel = max_kernel\n",
    "\n",
    "    def forward(self, input):\n",
    "        kernel_size = torch.randint(self.min_kernel, self.max_kernel, size=(1,1)).item()\n",
    "        # print(kernel_size)\n",
    "        N, C, H, W = input.shape\n",
    "        kernel = torch.ones((C, C, kernel_size, kernel_size))\n",
    "        out = F.conv2d(input=input, weight=kernel, padding=kernel_size//2, groups=C)\n",
    "        out = torch.clamp(out, 0, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertMaskTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, mask):\n",
    "        return 1 - mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_transforms = v2.Compose([\n",
    "    InvertMaskTransform(),\n",
    "    DilationTransform(7, 42),\n",
    "    v2.RandomCrop(image_dim),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    InvertMaskTransform()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_mean = (131.7364/255.0, 106.1529/255.0,  92.5201/255.0)\n",
    "celeb_std  = (77.2750/255.0, 70.0117/255.0, 68.6962/255.0)\n",
    "\n",
    "image_transforms = v2.Compose([\n",
    "    T.Normalize(celeb_mean, celeb_std),\n",
    "    T.Resize(image_dim, antialias=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebA_HQ(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len([name for name in os.listdir(self.img_dir) if os.path.isfile(os.path.join(self.img_dir, name))])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_dir + str(idx) + \".jpg\"\n",
    "        image = read_image(img_path).type(dtype)  # Already comes out as a tensor, don't need to use T.ToTensor()\n",
    "        image = image/255.0  #convert from 0-255 to 0-1\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, path, transform=None, preproc=False):\n",
    "        self.path = path\n",
    "        self.len = len(os.listdir(path))\n",
    "        self.transforms = transform\n",
    "        self.preproc = preproc\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < self.len, f\"Index {idx} is bigger than the number of elements {self.len}\"\n",
    "\n",
    "        if self.preproc:\n",
    "            img_path = os.path.join(self.path, f\"{idx}.png\")\n",
    "            img = plt.imread(img_path)\n",
    "            H, W, C = img.shape\n",
    "            if self.transforms is not None:\n",
    "                tensorized = torch.tensor(img).permute(2,0,1).view((1,C,H,W))\n",
    "                return self.transforms(tensorized).squeeze(1).expand(3, -1, -1)\n",
    "            else:\n",
    "                return torch.tensor(img).permute(2,0,1)\n",
    "        else:\n",
    "            img_path = os.path.join(self.path, f\"{idx:05}.png\")\n",
    "            img = plt.imread(img_path)\n",
    "            H, W = img.shape\n",
    "            if self.transforms is not None:\n",
    "                return self.transforms(torch.tensor(img).view((1, 1, H, W))).squeeze(1).expand(3, -1, -1)\n",
    "            else:\n",
    "                return torch.tensor(img).view((1, 1, H, W)).squeeze(1).expand(3, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_data = CelebA_HQ(\"./CelebA-HQ-img/\", transform=image_transforms)\n",
    "#celeb_data = CelebA_HQ(\".\\\\Datasets\\\\CelebAMask-HQ\\\\CelebA-HQ-img\\\\\", transform=image_transforms) #Christian local\n",
    "#celeb_data = CelebA_HQ(my_base_path + \"Celeb-Dataset/CelebA-HQ-img/\") #Colab version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dataset = MaskDataset('./irregular_mask/disocclusion_img_mask', transform=mask_transforms)\n",
    "#mask_dataset = MaskDataset('.\\\\Datasets\\\\irregular_mask\\\\disocclusion_img_mask', transform=mask_transforms) #Christian local"
    "\n",
    "mask_preprocset = MaskDataset('.\\\\Datasets\\\\masks_preprocessed', preproc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process Masks\n",
    "#from torchvision.utils import save_image\n",
    "\n",
    "'''\n",
    "mask_loader_process = DataLoader(mask_dataset, batch_size=1, num_workers=4, sampler=sampler.SubsetRandomSampler(range(39000,43000)))\n",
    "img_num = 27729 # Controls the saved file name\n",
    "num_cells = 512*512*3\n",
    "for t, x in enumerate(mask_loader_process):\n",
    "        N, C, H, W, = x.shape\n",
    "        x = x.reshape(C,H,W) # N=1\n",
    "        mask_open = torch.sum(x)/num_cells # How much of the original image is visible\n",
    "        if mask_open > 0.29 and mask_open < 0.965:\n",
    "            save_image(x, './Datasets/masks_preprocessed/'+str(img_num)+'.png')\n",
    "            img_num += 1\n",
    "        if t % 1000 == 0:\n",
    "            print(t)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN  = 26900\n",
    "NUM_VAL    = 600\n",
    "NUM_CELEB  = 30000\n",
    "batch_size = 2\n",
    "n_workers = 2  # Should use positive integer on Datahub. Use zero for local or it will \"exited unexpectedly\"\n",
    "\n",
    "\n",
    "loader_train = DataLoader(celeb_data, batch_size=batch_size, num_workers=n_workers, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "loader_val = DataLoader(celeb_data, batch_size=batch_size, num_workers=n_workers, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, NUM_TRAIN+NUM_VAL)))\n",
    "\n",
    "loader_test = DataLoader(celeb_data, batch_size=batch_size, num_workers=n_workers, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN+NUM_VAL, NUM_CELEB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_loader_train = DataLoader(mask_preprocset, batch_size=batch_size, num_workers=n_workers, sampler=sampler.SubsetRandomSampler(range(1,30001)))\n",
    "\n",
    "mask_loader_val = DataLoader(mask_preprocset, batch_size=batch_size, num_workers=n_workers, sampler=sampler.SubsetRandomSampler(range(30001, 31201)))\n",
    "\n",
    "mask_loader_test = DataLoader(mask_dataset, batch_size=batch_size, num_workers=n_workers, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN+NUM_VAL+1, 50000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding = 0, batch_norm=True, activation='relu') -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "        self.bn = nn.Identity()\n",
    "        if batch_norm:\n",
    "            self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "            \n",
    "        self.act = nn.Identity()\n",
    "        if activation == 'leaky_relu':\n",
    "            self.act = nn.LeakyReLU(0.2)\n",
    "        elif activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        assert x.shape == mask.shape, f\"Shape of x {x.shape} doesn't match shape of mask {mask.shape}\"\n",
    "        \n",
    "        # Pad x and mask\n",
    "        x1 = F.pad(x, pad=[self.padding]*4)\n",
    "        mask1 = F.pad(mask, pad=[self.padding]*4)\n",
    "\n",
    "        # Everything mask\n",
    "        sum_one = mask.shape[1] * self.kernel_size * self.kernel_size\n",
    "        mask_kernel = torch.ones((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size)).to(device)\n",
    "        # print(mask1.dtype, mask_kernel.dtype)\n",
    "        sum_mask = F.conv2d(mask1, weight=mask_kernel, stride=self.stride)\n",
    "        out_mask = (sum_mask > 1).type(dtype)\n",
    "\n",
    "        sum_mask = sum_mask.clamp(min=1e-10)\n",
    "\n",
    "        bias = self.conv.bias.view(1, -1, 1, 1)\n",
    "        y = self.conv(x1 * mask1)\n",
    "        y = (y - bias) * (sum_one / sum_mask) + bias      \n",
    "        y = self.act(self.bn(y))\n",
    "        return y, out_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding = 0, batch_norm=True, activation='relu') -> None:\n",
    "        super().__init__()\n",
    "        self.upsample = nn.UpsamplingNearest2d(scale_factor=2)\n",
    "        self.pconv = PConvBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, batch_norm=batch_norm, activation=activation)\n",
    "\n",
    "    def forward(self, x, mask, x2, mask2):\n",
    "        x1 = self.upsample(x)\n",
    "        mask1 = F.interpolate(mask, scale_factor=2, mode='nearest')\n",
    "        # mask1 = F.upsample_nearest(mask, scale_factor=2)\n",
    "        x3, mask3 = torch.cat([x1, x2], dim=1), torch.cat([mask1, mask2], dim=1)\n",
    "        return self.pconv(x3, mask3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pconv1 = PConvBlock(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, batch_norm=False, activation='relu')\n",
    "        self.pconv2 = PConvBlock(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=2, batch_norm=True, activation='relu')\n",
    "        self.pconv3 = PConvBlock(in_channels=128, out_channels=256, kernel_size=5, stride=2, padding=2, batch_norm=True, activation='relu')\n",
    "        self.pconv4 = PConvBlock(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1, batch_norm=True, activation='relu')\n",
    "        self.pconv5 = PConvBlock(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1, batch_norm=True, activation='relu')\n",
    "        self.pconv6 = PConvBlock(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1, batch_norm=True, activation='relu')\n",
    "        self.pconv7 = PConvBlock(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1, batch_norm=True, activation='relu')\n",
    "        self.pconv8 = PConvBlock(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1, batch_norm=True, activation='relu')\n",
    "\n",
    "        self.up1 = Up(in_channels=1024, out_channels=512, kernel_size=3, padding=1, stride=1, batch_norm=True, activation='leaky_relu')\n",
    "        self.up2 = Up(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=1, batch_norm=True, activation='leaky_relu')\n",
    "        self.up3 = Up(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=1, batch_norm=True, activation='leaky_relu')\n",
    "        self.up4 = Up(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=1, batch_norm=True, activation='leaky_relu')\n",
    "        self.up5 = Up(in_channels=768, out_channels=256, kernel_size=3, stride=1, padding=1, batch_norm=True, activation='leaky_relu')\n",
    "        self.up6 = Up(in_channels=384, out_channels=128, kernel_size=3, stride=1, padding=1, batch_norm=True, activation='leaky_relu')\n",
    "        self.up7 = Up(in_channels=192, out_channels=64, kernel_size=3, stride=1, padding=1, batch_norm=True, activation='leaky_relu')\n",
    "        self.up8 = Up(in_channels=67, out_channels=3, kernel_size=3, stride=1, padding=1, batch_norm=False, activation=None)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        x1, mask1 = self.pconv1(x, mask)\n",
    "        x2, mask2 = self.pconv2(x1, mask1)\n",
    "        x3, mask3 = self.pconv3(x2, mask2)\n",
    "        x4, mask4 = self.pconv4(x3, mask3)\n",
    "        x5, mask5 = self.pconv5(x4, mask4)\n",
    "        x6, mask6 = self.pconv6(x5, mask5)\n",
    "        x7, mask7 = self.pconv7(x6, mask6)\n",
    "        x8, mask8 = self.pconv8(x7, mask7)\n",
    "\n",
    "        _, _m = self.up1(x8, mask8, x7, mask7)\n",
    "        _, _m = self.up2(_, _m, x6, mask6)\n",
    "        _, _m = self.up3(_, _m, x5, mask5)\n",
    "        _, _m = self.up4(_, _m, x4, mask4)\n",
    "        _, _m = self.up5(_, _m, x3, mask3)\n",
    "        _, _m = self.up6(_, _m, x2, mask2)\n",
    "        _, _m = self.up7(_, _m, x1, mask1)\n",
    "        _, _m = self.up8(_, _m, x, mask)\n",
    "\n",
    "        return _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(name, features):\n",
    "        def hook(model, input, output):\n",
    "            features[name].append(output.detach())\n",
    "        return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InpaintingLoss(nn.Module):\n",
    "    def __init__(self, weights, trained_model='vgg') -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "        self.ref_model = None\n",
    "        self.ref_model_transforms = []\n",
    "        self.pool_layers = []\n",
    "\n",
    "        self.pool_outs = {\n",
    "            'pool1': [],\n",
    "            'pool2': [],\n",
    "            'pool3': []\n",
    "        }\n",
    "\n",
    "        if trained_model == 'vgg':\n",
    "            self._get_vgg_model()\n",
    "\n",
    "        self.ref_model = self.ref_model.to(device)\n",
    "        self.ref_model.eval()\n",
    "\n",
    "\n",
    "    def _get_vgg_model(self) -> None:\n",
    "        self.ref_model = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1)\n",
    "        self.ref_model_transforms = T.Compose([\n",
    "            T.Resize(256, antialias=True),\n",
    "            T.CenterCrop(224),\n",
    "            # T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.ref_model.eval()\n",
    "\n",
    "        self.pool_layers = []\n",
    "        for layer in (self.ref_model.named_modules()):\n",
    "            if type(layer[1]) == torch.nn.modules.pooling.MaxPool2d:\n",
    "                idx = int(layer[0].split('.')[1])\n",
    "                self.pool_layers.append(idx)\n",
    "\n",
    "        print(\"Following Pooling Layers found: \", self.pool_layers)\n",
    "        print(f\"Using 1st three {self.pool_layers[0:3]} for perceptual loss\")\n",
    "        self.pool_layers = self.pool_layers[0:3] # Updating the useful pool layer\n",
    "\n",
    "        # Adding forward_hooks\n",
    "        for i, layer_idx in enumerate(self.pool_layers):\n",
    "            self.ref_model.features[layer_idx].register_forward_hook(get_features(f\"pool{i+1}\", self.pool_outs))\n",
    "\n",
    "\n",
    "    def forward(self, input, out, gt, mask):\n",
    "        '''\n",
    "        input - N x C x H x W\n",
    "        out - N x C x H x W\n",
    "        gt - N x C x H x W\n",
    "        mask - N x C x H x W (or N x 1 x H x W, if this is the case then cast it to N x C x H x W)\n",
    "        '''\n",
    "        assert input.shape == out.shape, \"input and output are of different shapes\"\n",
    "        assert out.shape == gt.shape, \"out and gt are of different shapes\"\n",
    "        assert out.shape == mask.shape, \"out and mask are of different shapes\"\n",
    "        assert len(input.shape) == 4, \"Expected shape is N x C x H x W\"\n",
    "        N, C, H, W = input.shape\n",
    "        \n",
    "        self.pool_outs['pool1'] = []\n",
    "        self.pool_outs['pool2'] = []\n",
    "        self.pool_outs['pool3'] = []\n",
    "\n",
    "        # Pixel Losses\n",
    "        N_gt = C * H * W\n",
    "\n",
    "        loss_hole = torch.norm((out - gt) * (1 - mask), p=1, dim=(1,2,3)) / N_gt\n",
    "        loss_valid = torch.norm((out - gt) * mask, p=1, dim=(1,2,3)) / N_gt\n",
    "        # print(\"Loss Hole: \", loss_hole)\n",
    "        # print(\"Loss Valid: \", loss_valid)\n",
    "\n",
    "        loss_perceptual = 0\n",
    "        loss_style_out = 0\n",
    "        loss_style_comp = 0\n",
    "        with torch.no_grad():\n",
    "            comp = mask * gt + (1 - mask) * out\n",
    "            # Do not change this order. DO NOT.\n",
    "            _ = self.ref_model(self.ref_model_transforms(out))\n",
    "            _ = self.ref_model(self.ref_model_transforms(gt))\n",
    "            _ = self.ref_model(self.ref_model_transforms(comp))\n",
    "\n",
    "            for i in range(len(self.pool_layers)):\n",
    "                # Perceptual Loss\n",
    "                phsi = self.pool_outs[f'pool{i+1}']\n",
    "                phsi_out, phsi_gt, phsi_comp = phsi[0], phsi[1], phsi[2]\n",
    "                temp_perceptual_loss = 0\n",
    "\n",
    "                N, C, H, W = phsi_out.shape\n",
    "                count = C * H * W\n",
    "\n",
    "                loss_out_gt = torch.norm(phsi_out - phsi_gt, p = 1, dim=(1, 2, 3))\n",
    "                loss_comp_gt = torch.norm(phsi_comp - phsi_gt, p = 1, dim=(1, 2, 3))\n",
    "                temp_perceptual_loss = loss_out_gt + loss_comp_gt\n",
    "                loss_perceptual += (temp_perceptual_loss / count)\n",
    "\n",
    "                # Loss Style Out and Loss Style Comp\n",
    "                temp_loss = 0\n",
    "                phsi_out = phsi_out.view(N, C, -1) @ phsi_out.view(N, C, -1).transpose(1, 2)\n",
    "                phsi_comp = phsi_comp.view(N, C, -1) @ phsi_comp.view(N, C, -1).transpose(1, 2)\n",
    "                phsi_gt = phsi_gt.view(N, C, -1) @ phsi_gt.view(N, C, -1).transpose(1, 2)\n",
    "\n",
    "                temp_loss = torch.norm(phsi_out - phsi_gt, p=1, dim=(1,2))\n",
    "                loss_style_out += (temp_loss / (count * C * C))\n",
    "\n",
    "                temp_loss = torch.norm(phsi_comp - phsi_gt, p=1, dim=(1,2))\n",
    "                loss_style_comp += (temp_loss / (count * C * C))\n",
    "\n",
    "            comp_temp = torch.zeros_like(comp)\n",
    "            comp_temp[:, :, :-1, :] = comp_temp[:, :, 1:, :]\n",
    "            dx = comp_temp - comp\n",
    "            comp_temp[:, :, :, :-1] = comp_temp[:, :, :, 1:]\n",
    "            dy = comp_temp - comp\n",
    "\n",
    "            loss_tv = torch.norm(dx * (1 - mask), p = 1, dim=(1, 2, 3)) + torch.norm(dy * (1 - mask), p=1, dim=(1, 2, 3))\n",
    "            loss_tv /= N_gt\n",
    "\n",
    "            # print(\"Loss Hole: \", loss_hole)\n",
    "            # print(\"Loss Valid: \", loss_valid)\n",
    "            # print(\"Loss Perceptual: \", loss_perceptual)\n",
    "            # print(\"Loss Style Out: \", loss_style_out)\n",
    "            # print(\"Loss Style Comp: \", loss_style_comp)\n",
    "            # print(\"Loss TV: \", loss_tv)\n",
    "\n",
    "        total_loss = self.weights['hole'] * loss_hole + self.weights['valid'] * loss_valid + self.weights['perceptual'] * loss_perceptual + \\\n",
    "                        self.weights['style_out'] * loss_style_out + self.weights['style_comp'] * loss_style_comp + self.weights['tv'] * loss_tv\n",
    "\n",
    "        return total_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    \n",
    "def resume(model, filename):\n",
    "    if os.path.exists(filename):\n",
    "        model.load_state_dict(torch.load(filename))\n",
    "        print(\"Model Overridden with best available saved model\")\n",
    "    else:\n",
    "        print(\"No model found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader, mask_dataloader, loss_criterion):\n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "    model = model.to(device)\n",
    "    model.eval()  # set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x in val_dataloader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "\n",
    "            mask = next(iter(mask_dataloader))\n",
    "            mask = mask.to(device, dtype=dtype)\n",
    "            \n",
    "            out = model(x, mask)\n",
    "            loss = loss_criterion(x, out, x, mask)\n",
    "            \n",
    "            num_samples += x.size(0)\n",
    "            total_loss += loss.item()\n",
    "            # print(x.shape, num_samples, total_loss)\n",
    "\n",
    "        total_loss = total_loss / num_samples\n",
    "        print('Val Loss: ', (total_loss))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataloader, val_dataloader, train_mask_dataloader, val_mask_dataloader, \n",
    "          loss_criterion, epochs=1, finetuning=False):\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    model.train()  # put model into training mode\n",
    "    if finetuning:\n",
    "        # turn off BN accumulation in encoder layers\n",
    "        # by setting train=False\n",
    "        #model.pconv1.bn.train(False) #pconv1 has batchnorm=False\n",
    "        model.pconv2.bn.train(False)\n",
    "        model.pconv3.bn.train(False)\n",
    "        model.pconv4.bn.train(False)\n",
    "        model.pconv5.bn.train(False)\n",
    "        model.pconv6.bn.train(False)\n",
    "        model.pconv7.bn.train(False)\n",
    "        model.pconv8.bn.train(False)\n",
    "\n",
    "    # Tracking\n",
    "    BN_mean_E2 = model.pconv2.bn.running_mean # Encoder Layer 2\n",
    "    BN_var_E2  = model.pconv2.bn.running_var\n",
    "    BN_mean_E7 = model.pconv7.bn.running_mean # Encoder Layer 7\n",
    "    BN_var_E7  = model.pconv7.bn.running_var\n",
    "    BN_mean_D2 = model.up2.pconv.bn.running_mean # Decoder Layer 2\n",
    "    BN_var_D2  = model.up2.pconv.bn.running_var\n",
    "    BN_mean_D7 = model.up7.pconv.bn.running_mean # Decoder Layer 7\n",
    "    BN_var_D7  = model.up7.pconv.bn.running_var\n",
    "\n",
    "\n",
    "    # Checkpointing\n",
    "    best_model = None\n",
    "    best_val_loss = 1e15\n",
    "\n",
    "    for e in range(epochs):\n",
    "        tic = time.perf_counter()\n",
    "        for t, x in enumerate(train_dataloader):\n",
    "            x = x.to(device=device, dtype=dtype)  \n",
    "            \n",
    "            mask = next(iter(train_mask_dataloader))\n",
    "            mask = mask.to(device, dtype=dtype)\n",
    "\n",
    "            out = model(x, mask)\n",
    "            loss = loss_criterion(x, out, x, mask)\n",
    "\n",
    "            #print(f\"Minibatch {t}, Loss: \", loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t + 1, loss.item()))\n",
    "                val_loss = evaluate(model, val_dataloader, val_mask_dataloader, loss_criterion)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model = model\n",
    "                    checkpoint(best_model, \"./best_model\")\n",
    "                model.train()  # put model back into to training mode post-validation\n",
    "                \n",
    "                E2_flag = False\n",
    "                E7_flag = False\n",
    "                D2_flag = False\n",
    "                D7_flag = False\n",
    "                if not torch.equal(BN_mean_E2, model.pconv2.bn.running_mean):\n",
    "                    E2_flag = True\n",
    "                if not torch.equal(BN_var_E2, model.pconv2.bn.running_var):\n",
    "                    E2_flag = True \n",
    "                if E2_flag:\n",
    "                    print(\"Encoder Layer 2 BatchNorm running avg changed\")\n",
    "                if not torch.equal(BN_mean_E7, model.pconv7.bn.running_mean):\n",
    "                    E7_flag = True\n",
    "                if not torch.equal(BN_var_E7, model.pconv7.bn.running_var):\n",
    "                    E7_flag = True \n",
    "                if E7_flag:\n",
    "                    print(\"Encoder Layer 7 BatchNorm running avg changed\")\n",
    "                if not torch.equal(BN_mean_D2, model.up2.pconv.bn.running_mean):\n",
    "                    D2_flag = True\n",
    "                if not torch.equal(BN_var_D2, model.up2.pconv.bn.running_var):\n",
    "                    D2_flag = True \n",
    "                if D2_flag:\n",
    "                    print(\"Decoder Layer 2 BatchNorm running avg changed\")\n",
    "                if not torch.equal(BN_mean_D7, model.up7.pconv.bn.running_mean):\n",
    "                    D7_flag = True\n",
    "                if not torch.equal(BN_var_D7, model.up7.pconv.bn.running_var):\n",
    "                    D7_flag = True \n",
    "                if D7_flag:\n",
    "                    print(\"Decoder Layer 7 BatchNorm running avg changed\")\n",
    "                BN_mean_E2 = model.pconv2.bn.running_mean\n",
    "                BN_var_E2  = model.pconv2.bn.running_var\n",
    "                BN_mean_E7 = model.pconv7.bn.running_mean\n",
    "                BN_var_E7  = model.pconv7.bn.running_var\n",
    "                BN_mean_D2 = model.up2.pconv.bn.running_mean\n",
    "                BN_var_D2  = model.up2.pconv.bn.running_var\n",
    "                BN_mean_D7 = model.up7.pconv.bn.running_mean\n",
    "                BN_var_D7  = model.up7.pconv.bn.running_var\n",
    "                if finetuning:\n",
    "                    # turn off BN accumulation in encoder layers\n",
    "                    # by setting train=False\n",
    "                    #model.pconv1.bn.train(False) #pconv1 has batchnorm=False\n",
    "                    model.pconv2.bn.train(False)\n",
    "                    model.pconv3.bn.train(False)\n",
    "                    model.pconv4.bn.train(False)\n",
    "                    model.pconv5.bn.train(False)\n",
    "                    model.pconv6.bn.train(False)\n",
    "                    model.pconv7.bn.train(False)\n",
    "                    model.pconv8.bn.train(False)\n",
    "                print()\n",
    "        toc = time.perf_counter()\n",
    "        print('Epoch %d took %.3f minutes\\n' % (e, (toc - tic)/60.0))\n",
    "        epoch_model = model\n",
    "        checkpoint(epoch_model, \"./epoch_model_\"+str(e))\n",
    "\n",
    "    resume(model, \"./best_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following Pooling Layers found:  [6, 13, 23, 33, 43]\n",
      "Using 1st three [6, 13, 23] for perceptual loss\n"
     ]
    }
   ],
   "source": [
    "weights = {\n",
    "    'hole': 6,\n",
    "    'valid': 1,\n",
    "    'perceptual': 0.05,\n",
    "    'style_out': 120,\n",
    "    'style_comp': 120,\n",
    "    'tv': 0.1\n",
    "}\n",
    "\n",
    "loss_criterion = InpaintingLoss(weights=weights, trained_model='vgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "\n",
    "# Scale learning rate based on batch size\n",
    "# In PConv paper, batch=6, lr = 0.0002\n",
    "# If using:\n",
    "# Batch = 3, lr = 0.0001 or 1e-4\n",
    "# Batch = 2, lr = 0.000066666 or 6.66666e-5\n",
    "# Batch = 1, lr = 0.000033333 or 3.33333e-5\n",
    "learning_rate = 0.0001\n",
    "fine_tuning_rate = 0.00005\n",
    "\n",
    "epochs = 2  # Assume 1 epoch = 90 minutes\n",
    "#########################\n",
    "\n",
    "model = PConvNet()\n",
    "\n",
    "resume(model, './best_model')\n",
    "\n",
    "model = model.to(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 100, loss = 0.3780\n",
      "Val Loss:  0.12965480744838714\n",
      "\n",
      "Epoch 0, Iteration 200, loss = 0.4272\n",
      "Val Loss:  0.1498851140588522\n",
      "\n",
      "Epoch 0, Iteration 300, loss = 0.3800\n",
      "Val Loss:  0.131889081671834\n",
      "\n",
      "Epoch 0, Iteration 400, loss = 0.3123\n",
      "Val Loss:  0.14738179437816143\n",
      "\n",
      "Epoch 0, Iteration 500, loss = 0.1894\n",
      "Val Loss:  0.13854100778698922\n",
      "\n",
      "Epoch 0, Iteration 600, loss = 0.3311\n",
      "Val Loss:  0.1559525553882122\n",
      "\n",
      "Epoch 0, Iteration 700, loss = 0.4217\n",
      "Val Loss:  0.14764194279909135\n",
      "\n",
      "Epoch 0, Iteration 800, loss = 0.1507\n",
      "Val Loss:  0.13652860306203365\n",
      "\n",
      "Epoch 0, Iteration 900, loss = 0.4381\n",
      "Val Loss:  0.1456103242561221\n",
      "\n",
      "Epoch 0, Iteration 1000, loss = 0.4086\n",
      "Val Loss:  0.15001410245895386\n",
      "\n",
      "Epoch 0, Iteration 1100, loss = 0.6767\n",
      "Val Loss:  0.14248811826109886\n",
      "\n",
      "Epoch 0, Iteration 1200, loss = 0.3549\n",
      "Val Loss:  0.12735597372055055\n",
      "\n",
      "Epoch 0, Iteration 1300, loss = 0.3243\n",
      "Val Loss:  0.14112549014389514\n",
      "\n",
      "Epoch 0, Iteration 1400, loss = 0.2540\n",
      "Val Loss:  0.13276547193527222\n",
      "\n",
      "Epoch 0, Iteration 1500, loss = 0.3127\n",
      "Val Loss:  0.13759970106184483\n",
      "\n",
      "Epoch 0, Iteration 1600, loss = 0.2196\n",
      "Val Loss:  0.14526181928813459\n",
      "\n",
      "Epoch 0, Iteration 1700, loss = 0.4321\n",
      "Val Loss:  0.13249876022338866\n",
      "\n",
      "Epoch 0, Iteration 1800, loss = 0.2865\n",
      "Val Loss:  0.1366092362254858\n",
      "\n",
      "Epoch 0, Iteration 1900, loss = 0.3136\n",
      "Val Loss:  0.142800213098526\n",
      "\n",
      "Epoch 0, Iteration 2000, loss = 0.1765\n",
      "Val Loss:  0.13357773497700692\n",
      "\n",
      "Epoch 0, Iteration 2100, loss = 0.0689\n",
      "Val Loss:  0.13416406609117984\n",
      "\n",
      "Epoch 0, Iteration 2200, loss = 0.5965\n",
      "Val Loss:  0.13967638745903968\n",
      "\n",
      "Epoch 0, Iteration 2300, loss = 0.2957\n",
      "Val Loss:  0.13480126567184925\n",
      "\n",
      "Epoch 0, Iteration 2400, loss = 0.4520\n",
      "Val Loss:  0.15101260624825955\n",
      "\n",
      "Epoch 0, Iteration 2500, loss = 0.4996\n",
      "Val Loss:  0.13531917199492455\n",
      "\n",
      "Epoch 0, Iteration 2600, loss = 0.3936\n",
      "Val Loss:  0.15113126255571843\n",
      "\n",
      "Epoch 0, Iteration 2700, loss = 0.1731\n",
      "Val Loss:  0.12836138803511857\n",
      "\n",
      "Epoch 0, Iteration 2800, loss = 0.2056\n",
      "Val Loss:  0.13920092582702637\n",
      "\n",
      "Epoch 0, Iteration 2900, loss = 0.1122\n",
      "Val Loss:  0.14807992801070213\n",
      "\n",
      "Epoch 0, Iteration 3000, loss = 0.3500\n",
      "Val Loss:  0.14524260751903056\n",
      "\n",
      "Epoch 0, Iteration 3100, loss = 0.4001\n",
      "Val Loss:  0.12927466399967671\n",
      "\n",
      "Epoch 0, Iteration 3200, loss = 0.5229\n",
      "Val Loss:  0.14203539036214352\n",
      "\n",
      "Epoch 0, Iteration 3300, loss = 0.5431\n",
      "Val Loss:  0.14420235693454742\n",
      "\n",
      "Epoch 0, Iteration 3400, loss = 0.3869\n",
      "Val Loss:  0.14893954955041408\n",
      "\n",
      "Epoch 0, Iteration 3500, loss = 0.1287\n",
      "Val Loss:  0.1368523735553026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = train(model=model, \n",
    "              optimizer=optimizer, \n",
    "              train_dataloader=loader_train, \n",
    "              val_dataloader=loader_val, \n",
    "              train_mask_dataloader=mask_loader_train, \n",
    "              val_mask_dataloader=mask_loader_val, \n",
    "              loss_criterion=loss_criterion, \n",
    "              epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_129/3446998537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_loader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_129/2019429922.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, val_dataloader, mask_dataloader, loss_criterion)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_129/1743036476.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpconv6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpconv7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mx8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpconv8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_129/1564909926.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Everything mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msum_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mmask_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m# print(mask1.dtype, mask_kernel.dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0msum_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(model, loader_test, mask_loader_test, loss_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resume(model, './best_model')\n",
    "model.eval()\n",
    "imgs, masks = next(iter(loader_test)).to(device), next(iter(mask_loader_test)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(imgs, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "temp_img = imgs[idx].transpose(0, 1).transpose(1, 2).cpu()\n",
    "temp_mask = masks[idx].transpose(0, 1).transpose(1, 2).cpu()\n",
    "temp_out = out[idx].transpose(0, 1).transpose(1, 2).cpu().detach().numpy()\n",
    "\n",
    "f = plt.figure(figsize=(12, 8))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(temp_img)\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(temp_mask)\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(temp_mask * temp_img)\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(temp_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
